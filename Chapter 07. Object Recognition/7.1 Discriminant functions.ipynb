{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Discriminant functions\n",
    "\n",
    "In the previous chapter, we explored how to extract a feature vector $\\textbf{x} = [x_1, x_2, \\ldots, x_n]$ to describe an image object or *region* (e.g. a binary region, a segmented image, $\\ldots$). In this chapter, our goal is **to classify the object as belonging to one of $M$ object classes or categories**, according with its feature vector $\\textbf{x}$. For that, the space defined by the features in $\\textbf{x}$, also referred as **feature space**, is divided into prediction regions. The image below shows an example of those regions in a scenario with categories $C=\\{C_1,C_2,C_3\\}$, so $M=3$, and features $\\textbf{x}=[x_1,x_2]$:\n",
    "$\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/clas-space.png\" width=\"300\">$\\\\[3pt]$\n",
    "\n",
    "Two main steps are needed in order to build an object recognition system and work with it:\n",
    "\n",
    "- A **training (design) phase**, where sample vectors of known objects are used to learn the classifier (supervised learning).\n",
    "- A **prediction (online) phase**, where the image objects are classified to one of the classed based on the learned prediction models.\n",
    "\n",
    "Such prediction models come in two flavors:\n",
    "\n",
    "- **Statistical classifiers:**\n",
    "  - It is assumed that the feature vectors $\\textbf{x}$ of the classes $C$ follow a **statistical distribution**.\n",
    "  - The parameters of such distribution need to be **learned from known objects**.\n",
    "  - Examples: Naïve Bayessian Classifier, Logistic Regression, Conditional Random Fields.  \n",
    "- **Non-statistical classifiers:**\n",
    "  - No assumption is made on the statistical distribution of the feature vector.\n",
    "  - The coefficient of deterministic discriminant functions are learned.\n",
    "  - Examples: Support Vector Machine, Perceptron, AdaBoost.\n",
    "\n",
    "This notebook focuses on non-statistical classifiers. Concretely it covers:\n",
    "\n",
    "- Linear discriminant functions (section 7.1.1).\n",
    "- Separability (section 7.1.2).\n",
    "- Generalized discriminant function (section 7.1.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context - Traffic sign recognition\n",
    "\n",
    "Before self-driving vehicles can truly operate autonomously, they would need to identify traffic signs like \"speed limit\", \"children\" or \"turn ahead\". This problem is called **Traffic Sign Recognition (TSR)** and is faced by companies like Tesla or Google (Waymo), and now by *AliquindoiCars*, a startup located in the Andalusia Technology Park in Málaga.\n",
    "\n",
    "This is part of the features collectively called *Advanced Driver Assistance Systems (ADAS)*. It uses image processing techniques to detect the traffic signs. The detection methods can be generally divided into color based, shape based and learning based methods. $\\\\[3pt]$\n",
    "\n",
    "<img src=\"./images/signs.jpg\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "Given your growing experience in computer vision, *AliquindoiCars* contacted you asking for a TSR technique to be integrated into a self-driving car. \n",
    "\n",
    "### The data\n",
    "\n",
    "Since most traffic signs in Spain (and in the rest of the world) have a rectangular, circular or triangular shape, they provided you 3 folders containing 20 images each;\n",
    "\n",
    "- `./images/circles/{0-19}.png`\n",
    "- `./images/rectangles/{0-19}.png`\n",
    "- `./images/triangles/{0-19}.png`\n",
    "\n",
    "These folders contain segmented and binarized images of traffic signs captured by a camera in a car. **Our task is to design a recognition system able to distinguish each type of shape.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "\n",
    "images_path = './images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing feature vectors\n",
    "\n",
    "Prior to describe the methods that will be used to perform recognition, let's build the feature vectors they are going to use. For this, and given your experience, you are going to use the Hu Moments (7-size feature vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1</i></b></span>**\n",
    "\n",
    "**Your first task is** to obtain a $20x7$ matrix for each type of shape, where rows index images and columns Hu moments. In this way, the $5^{th}$ row in the matrix characterizing rectangular signs would contain the Hu moments computed from the $5^{th}$ image with that shape. \n",
    "\n",
    "In order to get a first impression of how these features are distributed in the feature space for the different shapes, plot the results using [`plt.scatter()`](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.scatter.html) (show only the firsts 2 Hu moments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "def image_moments(region):\n",
    "    \"\"\" Compute moments of the external contour in a binary image.   \n",
    "    \n",
    "        Args:\n",
    "            region: Binary image\n",
    "                    \n",
    "        Returns: \n",
    "            moments: dictionary containing all moments of the region\n",
    "    \"\"\"   \n",
    "    \n",
    "    # Get external contour\n",
    "    \n",
    "    \n",
    "    # Compute moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Hu moments for circle shape\n",
    "\n",
    "\n",
    "# Compute Hu moments for square shape\n",
    "\n",
    "\n",
    "# Compute Hu moments hor triangle shape\n",
    "\n",
    "\n",
    "# Define plot axis\n",
    "\n",
    "\n",
    "# Plot firsts two Hu moments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude\n",
    "\n",
    "**You must save the computed Hu moments of the segmented data**, since we are going to need them in the next notebook. For saving numPy matrices, you can use [`np.save()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html) (data is saved at `./data/` by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/hu_circles.npy\",hu_circles)\n",
    "np.save(\"./data/hu_triangles.npy\",hu_triangles)\n",
    "np.save(\"./data/hu_squares.npy\",hu_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.1 Linear discriminant functions\n",
    "\n",
    "Taking a look at the scatter plot resultant from the previous assignment, we can see how the three types of signs could be segmented using 2 lines. For example:\n",
    "\n",
    "<img src=\"./images/plot_linear.jpg\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "And that's what **linear discriminant functions** do! As we know, a way to define a 2D line is:\n",
    "\n",
    "$\\hspace{2cm} 0 = Ax + By + C\\\\[3pt]$  \n",
    "Using another notation:$\\\\[5pt]$  \n",
    "$\\hspace{2cm}0 = w_1x_1 + w_2x_2 + w_3$$\\\\[3pt]$  \n",
    "\n",
    "Note that we can generalize this 2D line to any dimension hyperplane:$\\\\[5pt]$  \n",
    "$\\hspace{2cm}0 = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + w_{n+1}\\ =\\ \\sum_{i=1}^{n+1}w_ix_i$$ \\ =\\ w^T\\cdot x\\\\[1pt]$  \n",
    "In this way, we can divide the feature space for any dimension (e.g. n = 7 for Hu moments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with two classes\n",
    "\n",
    "If we are considering just two classes $C=\\{C_1,C_2\\}$, we can define a linear discriminant function as:\n",
    "\n",
    "$\\hspace{2cm} d(\\mathbf{x}) = \\mathbf{w}^T \\cdot \\mathbf{x} \\ \\ \\ \\ \\ \\text{(dot product of both vectors)}$\n",
    "\n",
    "Formally, the vector $w$ is called **weight vector** and we will see a way to compute it automatically from a set of data called **training data**.\n",
    "\n",
    "In this way, when we have a new object characterized by $\\mathbf{x'}$ to classify, $d(\\mathbf{x'})$ is computed and:\n",
    "- If $d(\\mathbf{x'})\\ge 0$ the new object is assigned $C_1$, and\n",
    "- if $d(\\mathbf{x'})\\lt 0$ the object is assigned to $C_2$.\n",
    "\n",
    "We say that the linear discriminant function $d(\\mathbf{x'})$ works as a **decision boundary** between both classes. The following figure illustrates this (blue dots represent characterized objects belonging to $C_1$, while red ones those belonging to $C_2$):\n",
    "\n",
    "<img src=\"./images/linear_discriminant_function_2_categories.png\" width=\"250\">$\\\\[3pt]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2</i></b></span>**\n",
    "\n",
    "Let's play a bit with a toy example of a linear discriminant function. Consider that the learned weights during the training phase of the classifier are $\\mathbf{w} = [1,0,-3]$, so the linear discriminant function looks like $d(\\mathbf{x}) = 1x_1 + 0x_2 -3$. \n",
    "\n",
    "**Your task is** to classify some 2D data using the previous discriminant function, plotting them with different colors depending on their assigned class. \n",
    "\n",
    "*Tip: [how to color a data point using its class](https://stackoverflow.com/questions/43579626/pandas-plot-with-positive-values-one-color-and-negative-values-another)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define weight vector\n",
    "\n",
    "\n",
    "# Define data matrix\n",
    "\n",
    "\n",
    "# Compute class for each data\n",
    "\n",
    "\n",
    "# Plot classified data\n",
    "\n",
    "\n",
    "# Plot discriminant line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenarios with N-classes\n",
    "\n",
    "The previous idea can be generalized to an arbitrary number of classes, defining in that case hyper-planes in a n-dimensional space:\n",
    "\n",
    "$\\hspace{2cm}d(\\mathbf{x}) = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + w_{n+1}\\ =\\ \\mathbf{w}^T\\cdot \\mathbf{x}\\\\[1pt]$  \n",
    "\n",
    "In this case, the object is assigned to the category which discriminant function returns the highest value. That is:\n",
    "\n",
    "<img src=\"./images/linear_discriminant_functions_general_case.png\" width=\"400\">$\\\\[3pt]$\n",
    "\n",
    "When more than 2 classes are considered, the discriminant functions are no longer the decision boundaries between classes. However, the decision boundary between classes $C_i$ and $C_j$ can be computed as:\n",
    "\n",
    "$\\hspace{2cm}d_{ij}(\\mathbf{x}) = d_i(\\mathbf{x})-d_j(\\mathbf{x})=(\\mathbf{w}_i^T-\\mathbf{x}_j^T) \\cdot \\mathbf{x} = \\mathbf{w}_{ij}^T \\cdot \\mathbf{x}$\n",
    "\n",
    "The following image shows an example with 3 classes (notice that it also exists a decision boundary between classes 1 and 3, $d_{13}(\\mathbf{x})$, but it has been omitted for clarity).\n",
    "\n",
    "<img src=\"./images/2lines-discriminant.png\" width=\"400\">$\\\\[3pt]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 Separability\n",
    "\n",
    "An important concept regarding object recognition is such of **separability**. In this way, two set of points defining two classes may be separable or not in a given dimension (e.g. $x_1$, $x_2$, etc.).  \n",
    "\n",
    "Two sets are **linearly separable** if it exists at least one line in the plane that leaves all the points belonging to one class on one side of the line and all those belonging to the second class on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane. \n",
    "\n",
    "Some illustrative examples of this:\n",
    "\n",
    "<img src=\"./images/separability-2.png\" width=\"900\">$\\\\[3pt]$\n",
    "\n",
    "- As you can see, those classes are separable. Also, if only dimension $x_1$ is used, we can separate the classes as well. This is not recomended because there isn't a clear separation using only $x_1$, and $x_2$ is also **discriminative**, that is, it also provides valuable information for separating both classes.\n",
    "\n",
    "<img src=\"./images/separability-1.png\" width=\"900\">$\\\\[3pt]$\n",
    "\n",
    "- In this example the classes are not separable as they overlap in both dimensions. Additionally, we can see that $x_1$ is not discriminative as it doesn't provide any information for separating those classes. The addition of a third dimension/feature could fix this!\n",
    "\n",
    "<img src=\"./images/separability-3.png\" width=\"900\">$\\\\[3pt]$\n",
    "\n",
    "- This is an example of a non-linear separable problem. The classes can't be separated using a line, but a **generalized discriminant function** could do it. We will see this later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to sign shape recognition\n",
    "\n",
    "Back to our traffic sign shape recognition problem, we tried to separate the usual shapes of traffic signs using the Hu moments. As a first approximation to this problem, we plotted the first and second Hu moments ($x_1$ and $x_2$) in order to analyze them. You should have obtained something like this:\n",
    "\n",
    "<img src=\"./images/separability-4.png\" width=\"500\">$\\\\[3pt]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b><i>Thinking about it (1)</i></b></font>\n",
    "\n",
    "**Now you are in a good position to answer these questions:**\n",
    "\n",
    "- Is this problem linearly separable?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Are the classes separable using only the first Hu moment ($x_1$)?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Are the classes separable using only the second Hu moment ($x_2$)?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Which dimension is more discriminative?\n",
    "\n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>\n",
    "    \n",
    "- Would be *AliquindoiCars* guys happy if we propose a solution for shape classification based on linear discriminant functions?\n",
    "  \n",
    "    <p style=\"margin: 4px 0px 6px 5px; color:blue\"><i>Your answer here!</i></p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Generalized discriminant function\n",
    "\n",
    "As we saw above, some classes are not separable using a linear discriminant function. In those cases, we may need a **linear basis function model**, also called **generalized discriminant function** (notice that, despite of this, the problem may still not be separable).\n",
    "\n",
    "The idea behind these discriminant functions is to transform the $\\textbf{x}$ space into a $\\textbf{x' = f(x)}$ space ($dim(x) \\lt dim(x')$):$\\\\[5pt]$\n",
    "\n",
    "$\\hspace{2cm} \\scriptsize   d(x) = w_1\\ \\color{blue}{f_1(x)} + w_2\\ \\color{blue}{f_2(x)} + \\ldots + w_k\\ \\color{blue}{f_k(x)} + w_{k+1} = \\sum_{i=1}^{k+1}w_i\\ \\underbrace{\\color{blue}{f_i(x)}}_{\\scriptsize\\text{New space}}=\\mathbf{w}^T\\cdot \\mathbf{f}(\\mathbf{x})$\n",
    "\n",
    "$\\hspace{2.75cm}  \\scriptsize = w_1\\ \\color{blue}{x_1'} + w_2\\ \\color{blue}{x_2'} + \\ldots + w_k\\ \\color{blue}{x_k'} + w_{k+1} = \\sum_{i=1}^{k+1}w_i\\ \\color{blue}{x_i'} = w^T \\cdot \\color{blue}{x'}=d'(\\mathbf{x}')$\n",
    "\n",
    "Thereby:\n",
    "\n",
    "$$\n",
    "\\left.\\begin{aligned}\n",
    "        \\mathbf{x'} = \\mathbf{f}(\\mathbf{x}) &= [f_1(\\mathbf{x}) \\ \\ \\ f_2(\\mathbf{x}) \\ \\ \\ \\cdots \\ \\ \\ f_k(\\mathbf{x}) \\ \\ \\  1]^T\\\\\n",
    "        \\mathbf{w} &= [w_1 \\ \\ \\ w_2 \\ \\ \\ \\cdots \\ \\ \\ w_k \\ \\ \\ w_{k+1}]^T\n",
    "       \\end{aligned}\n",
    " \\right\\}\n",
    " \\ \\  d(\\mathbf{x})= \\mathbf{w}^T \\cdot \\mathbf{x} = \\mathbf{w}^T \\cdot \\mathbf{f}(\\mathbf{x})\n",
    "$$\n",
    " \n",
    " being $\\mathbf{f}(\\mathbf{x})$ the basis functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3</i></b></span>**\n",
    "\n",
    "Let's practice this transformation between spaces. **What to do?** You have to complete the method `transform_space()`, which transforms a set of 2D points into a new space defined by:\n",
    "\n",
    "$$x' = [x_1^2, \\ x_1x_2, \\ x_2^2, \\ x_1, \\ x_2, \\ 1]$$\n",
    "\n",
    "The input set of points is a $[n\\_data,2]$ shape matrix and the output is a $[n\\_data,6]$ shape matrix, being $n\\_data$ the number of points in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "def transform_space(data_points):\n",
    "    \"\"\" Compute moments of the external contour in a binary image.   \n",
    "    \n",
    "        Args:\n",
    "            data_points: Input matrix containing a set of 2D points\n",
    "                    \n",
    "        Returns: \n",
    "            fx: set of points transformed to a quadratic space\n",
    "    \"\"\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use next code to **test if the results are correct**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[2,3],[1,2],[2,7],[2,4],[1,5],[2,4]])\n",
    "\n",
    "x = transform_space(data)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "     [[ 4.  4.  4.  2.  2.  1.]\n",
    "     [ 1.  2.  4.  1.  2.  1.]\n",
    "     [ 4.  2.  1.  2.  1.  1.]\n",
    "     [25. 20. 16.  5.  4.  1.]\n",
    "     [16. 20. 25.  4.  5.  1.]\n",
    "     [16. 16. 16.  4.  4.  1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4</i></b></span>**\n",
    "\n",
    "Finally, we will see a toy example of a non-linear discriminant function in action. Concretely, we will consider a quadratic function, $d(x)=x_1^2-5x_2$ (so $k=5$), in a two classes problem with $\\mathbf{x}=[x_1 \\ \\ x_2]$. Thereby, the vector of features in the new space is:\n",
    "\n",
    "$\\hspace{2cm}  \\mathbf{x'}=[x_1^2 \\ \\ x_1x_2 \\ \\ x_2^2 \\ \\ x_1 \\ \\ x_2 \\ \\ 1]^T $ <br />\n",
    "so:\n",
    "- $x_1^2=f_1(\\mathbf{x})=x_1'$\n",
    "- $x_1 x_2=f_2(\\mathbf{x})=x_2'$\n",
    "- and so on.\n",
    "\n",
    "and the vector of weights turns out to be:\n",
    "\n",
    "$\\hspace{2cm} \\mathbf{w}=[1,0,0,0,-5,0]$.\n",
    "\n",
    "As for the new discriminant function we get:\n",
    "\n",
    "$\\hspace{2cm} d(\\mathbf{x}) = w_1 \\cdot x_1' + w_5 \\cdot x_5'$\n",
    "\n",
    "**Your task is to classify some 2D data** using the previous discriminant function and plot them coloring each point with a color depending on its class.\n",
    "\n",
    "*Tip: Remember to transform the data space before applying the dot product.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 4\n",
    "\n",
    "# Define weight vector\n",
    "\n",
    "\n",
    "# Define data matrix\n",
    "\n",
    "\n",
    "# Trasform from the x space to a f(x) space\n",
    "\n",
    "\n",
    "# Compute class for each data\n",
    "\n",
    "\n",
    "# Plot classified data\n",
    "\n",
    "\n",
    "# Plot discriminant line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Well done! This was a mostly theoretical notebook, but it described the basis of object classification. You have learned so far:\n",
    "\n",
    "- how linear and non-linear discriminant functions work,\n",
    "- to classify objects in a scenario with an arbitrary number of belonging classes using multiple discriminant functions, and\n",
    "- to check if a feature space allows for the definition of discriminative classifiers looking at the separability between classes.\n",
    "\n",
    "In the following notebooks in this chapter we will see how to automatically retrieve the weight vector from a set o training data. Exciting, isn't?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "188.85px",
    "left": "1530px",
    "right": "20px",
    "top": "120px",
    "width": "303px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
